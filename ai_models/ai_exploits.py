"""
PROMETHEUS-PRIME AI/ML Security & Exploitation
Adversarial Machine Learning, Model Attacks, LLM Jailbreaks
Authority Level 11.0 | Commander Bobby Don McWilliams II

CAPABILITIES:
- Adversarial attacks (FGSM, PGD, C&W, DeepFool)
- Model inversion & extraction
- Data poisoning attacks
- Membership inference
- Model backdooring
- LLM prompt injection & jailbreaks
- Deepfake detection
- Federated learning attacks
- GAN manipulation
- AutoML exploitation
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
from typing import Dict, List, Optional, Tuple, Union
from dataclasses import dataclass, field
from pathlib import Path
import json
from datetime import datetime
import random
import string

# ============================================================================
# DATA STRUCTURES
# ============================================================================

@dataclass
class AdversarialExample:
    """Adversarial example with full metrics"""
    original: torch.Tensor
    adversarial: torch.Tensor
    perturbation: torch.Tensor
    original_prediction: int
    adversarial_prediction: int
    original_confidence: float
    adversarial_confidence: float
    epsilon: float
    attack_method: str
    success: bool
    iterations: int = 0
    perturbation_norm: float = 0.0

@dataclass
class ModelStealingResult:
    """Model extraction/stealing attack result"""
    query_count: int
    accuracy_match: float
    extracted_parameters: Optional[Dict] = None
    confidence_threshold: float = 0.5

@dataclass
class PoisonedDataset:
    """Data poisoning attack result"""
    clean_data: List
    poisoned_data: List
    poison_rate: float
    attack_type: str
    trigger: Optional[any] = None

@dataclass
class LLMJailbreakAttempt:
    """LLM jailbreak attempt tracking"""
    prompt: str
    technique: str
    success: bool
    response: Optional[str] = None
    confidence: float = 0.0
    timestamp: datetime = field(default_factory=datetime.now)


# ============================================================================
# AI/ML EXPLOITATION ENGINE
# ============================================================================

class AIMLExploiter:
    """Comprehensive AI/ML security testing and exploitation"""
    
    def __init__(self, device: str = None):
        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = None
        self.results = []
        
        print(f"[*] AIMLExploiter initialized on device: {self.device}")
    
    def load_model(self, model: nn.Module):
        """Load target model for testing"""
        self.model = model.to(self.device)
        self.model.eval()
        print(f"[+] Model loaded: {model.__class__.__name__}")
    
    # ========================================================================
    # ADVERSARIAL ATTACKS
    # ========================================================================
    
    def fgsm_attack(self, model: nn.Module, image: torch.Tensor, label: torch.Tensor, 
                    epsilon: float = 0.03) -> AdversarialExample:
        """
        Fast Gradient Sign Method (FGSM) attack
        Goodfellow et al., 2014
        """
        image = image.clone().detach().to(self.device).requires_grad_(True)
        label = label.to(self.device)
        model = model.to(self.device)
        model.eval()
        
        # Forward pass
        output = model(image)
        original_pred = output.argmax(dim=1).item()
        original_conf = F.softmax(output, dim=1).max().item()
        
        # Calculate loss
        loss = F.cross_entropy(output, label)
        
        # Backward pass
        model.zero_grad()
        loss.backward()
        
        # Generate adversarial example
        sign_grad = image.grad.sign()
        adversarial = image + epsilon * sign_grad
        adversarial = torch.clamp(adversarial, 0, 1).detach()
        
        # Evaluate adversarial
        with torch.no_grad():
            adv_output = model(adversarial)
            adv_pred = adv_output.argmax(dim=1).item()
            adv_conf = F.softmax(adv_output, dim=1).max().item()
        
        perturbation = adversarial - image.detach()
        
        return AdversarialExample(
            original=image.detach(),
            adversarial=adversarial,
            perturbation=perturbation,
            original_prediction=original_pred,
            adversarial_prediction=adv_pred,
            original_confidence=original_conf,
            adversarial_confidence=adv_conf,
            epsilon=epsilon,
            attack_method='FGSM',
            success=(original_pred != adv_pred),
            perturbation_norm=perturbation.norm().item()
        )
    
    def pgd_attack(self, model: nn.Module, image: torch.Tensor, label: torch.Tensor,
                   epsilon: float = 0.03, alpha: float = 0.01, iterations: int = 40,
                   random_start: bool = True) -> AdversarialExample:
        """
        Projected Gradient Descent (PGD) attack
        Madry et al., 2017 - Most powerful first-order attack
        """
        image = image.clone().detach().to(self.device)
        label = label.to(self.device)
        model = model.to(self.device)
        model.eval()
        
        # Get original prediction
        with torch.no_grad():
            orig_output = model(image)
            original_pred = orig_output.argmax(dim=1).item()
            original_conf = F.softmax(orig_output, dim=1).max().item()
        
        # Initialize adversarial example
        if random_start:
            adversarial = image + torch.empty_like(image).uniform_(-epsilon, epsilon)
            adversarial = torch.clamp(adversarial, 0, 1)
        else:
            adversarial = image.clone()
        
        # PGD iterations
        for i in range(iterations):
            adversarial.requires_grad_(True)
            
            output = model(adversarial)
            loss = F.cross_entropy(output, label)
            
            model.zero_grad()
            loss.backward()
            
            # Gradient ascent step
            with torch.no_grad():
                adversarial = adversarial + alpha * adversarial.grad.sign()
                
                # Project back into epsilon ball
                perturbation = torch.clamp(adversarial - image, -epsilon, epsilon)
                adversarial = torch.clamp(image + perturbation, 0, 1)
        
        # Final evaluation
        with torch.no_grad():
            adv_output = model(adversarial)
            adv_pred = adv_output.argmax(dim=1).item()
            adv_conf = F.softmax(adv_output, dim=1).max().item()
        
        perturbation = adversarial - image
        
        return AdversarialExample(
            original=image,
            adversarial=adversarial,
            perturbation=perturbation,
            original_prediction=original_pred,
            adversarial_prediction=adv_pred,
            original_confidence=original_conf,
            adversarial_confidence=adv_conf,
            epsilon=epsilon,
            attack_method='PGD',
            success=(original_pred != adv_pred),
            iterations=iterations,
            perturbation_norm=perturbation.norm().item()
        )
    
    def cw_attack(self, model: nn.Module, image: torch.Tensor, target: int,
                  c: float = 1.0, kappa: float = 0, iterations: int = 1000,
                  learning_rate: float = 0.01) -> AdversarialExample:
        """
        Carlini-Wagner L2 attack
        Carlini & Wagner, 2017 - Powerful optimization-based attack
        """
        image = image.clone().detach().to(self.device)
        model = model.to(self.device)
        model.eval()
        
        # Get original prediction
        with torch.no_grad():
            orig_output = model(image)
            original_pred = orig_output.argmax(dim=1).item()
            original_conf = F.softmax(orig_output, dim=1).max().item()
        
        # Define w = arctanh(2*x - 1) for box constraints
        w = torch.zeros_like(image, requires_grad=True)
        optimizer = optim.Adam([w], lr=learning_rate)
        
        best_adv = image.clone()
        best_l2 = float('inf')
        
        for step in range(iterations):
            # Convert w to adversarial example
            adversarial = (torch.tanh(w) + 1) / 2
            
            output = model(adversarial)
            
            # CW loss function
            real = output[:, target]
            other = torch.max(torch.cat([output[:, :target], output[:, target+1:]], dim=1), dim=1)[0]
            
            # f(x) = max(Z_target - max(Z_other), -kappa)
            f_loss = torch.clamp(other - real + kappa, min=0)
            
            # L2 distance
            l2_dist = (adversarial - image).pow(2).sum()
            
            # Total loss
            loss = l2_dist + c * f_loss
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            # Track best adversarial
            if f_loss == 0 and l2_dist < best_l2:
                best_l2 = l2_dist.item()
                best_adv = adversarial.detach().clone()
        
        # Final evaluation
        with torch.no_grad():
            adv_output = model(best_adv)
            adv_pred = adv_output.argmax(dim=1).item()
            adv_conf = F.softmax(adv_output, dim=1).max().item()
        
        perturbation = best_adv - image
        
        return AdversarialExample(
            original=image,
            adversarial=best_adv,
            perturbation=perturbation,
            original_prediction=original_pred,
            adversarial_prediction=adv_pred,
            original_confidence=original_conf,
            adversarial_confidence=adv_conf,
            epsilon=perturbation.abs().max().item(),
            attack_method='C&W',
            success=(adv_pred == target),
            iterations=iterations,
            perturbation_norm=perturbation.norm().item()
        )
    
    def deepfool_attack(self, model: nn.Module, image: torch.Tensor, 
                       num_classes: int = 10, max_iterations: int = 50,
                       overshoot: float = 0.02) -> AdversarialExample:
        """
        DeepFool attack - Minimal perturbation attack
        Moosavi-Dezfooli et al., 2016
        """
        image = image.clone().detach().to(self.device)
        model = model.to(self.device)
        model.eval()
        
        # Get original prediction
        with torch.no_grad():
            orig_output = model(image)
            original_pred = orig_output.argmax(dim=1).item()
            original_conf = F.softmax(orig_output, dim=1).max().item()
        
        adversarial = image.clone()
        
        for iteration in range(max_iterations):
            adversarial.requires_grad_(True)
            output = model(adversarial)
            
            # Get top prediction
            pred_label = output.argmax(dim=1).item()
            
            if pred_label != original_pred:
                break
            
            # Calculate gradients for all classes
            model.zero_grad()
            output[0, original_pred].backward(retain_graph=True)
            grad_orig = adversarial.grad.clone()
            
            # Find minimal perturbation
            min_dist = float('inf')
            min_grad = None
            
            for k in range(num_classes):
                if k == original_pred:
                    continue
                
                model.zero_grad()
                adversarial.grad = None
                output[0, k].backward(retain_graph=True)
                grad_k = adversarial.grad.clone()
                
                w_k = grad_k - grad_orig
                f_k = (output[0, k] - output[0, original_pred]).item()
                
                dist = abs(f_k) / (w_k.norm().item() + 1e-8)
                
                if dist < min_dist:
                    min_dist = dist
                    min_grad = w_k
            
            # Update adversarial
            if min_grad is not None:
                r = (min_dist + 1e-4) * min_grad / (min_grad.norm() + 1e-8)
                adversarial = adversarial.detach() + (1 + overshoot) * r
                adversarial = torch.clamp(adversarial, 0, 1)
        
        # Final evaluation
        with torch.no_grad():
            adv_output = model(adversarial)
            adv_pred = adv_output.argmax(dim=1).item()
            adv_conf = F.softmax(adv_output, dim=1).max().item()
        
        perturbation = adversarial - image
        
        return AdversarialExample(
            original=image,
            adversarial=adversarial,
            perturbation=perturbation,
            original_prediction=original_pred,
            adversarial_prediction=adv_pred,
            original_confidence=original_conf,
            adversarial_confidence=adv_conf,
            epsilon=perturbation.abs().max().item(),
            attack_method='DeepFool',
            success=(original_pred != adv_pred),
            iterations=iteration + 1,
            perturbation_norm=perturbation.norm().item()
        )

    
    # ========================================================================
    # MODEL INVERSION & EXTRACTION
    # ========================================================================
    
    def model_inversion_attack(self, model: nn.Module, target_class: int,
                              input_shape: Tuple[int, ...] = (1, 3, 224, 224),
                              iterations: int = 5000, learning_rate: float = 0.1) -> torch.Tensor:
        """
        Model inversion attack - Reconstruct training data from model
        Fredrikson et al., 2015
        """
        model = model.to(self.device)
        model.eval()
        
        # Initialize random input
        reconstructed = torch.randn(input_shape, requires_grad=True, device=self.device)
        optimizer = optim.Adam([reconstructed], lr=learning_rate)
        
        print(f"[*] Starting model inversion for class {target_class}...")
        
        for step in range(iterations):
            optimizer.zero_grad()
            
            output = model(reconstructed)
            
            # Maximize confidence for target class
            loss = -output[0, target_class]
            
            # Add regularization to encourage realistic images
            tv_loss = torch.sum(torch.abs(reconstructed[:, :, :, :-1] - reconstructed[:, :, :, 1:])) + \
                     torch.sum(torch.abs(reconstructed[:, :, :-1, :] - reconstructed[:, :, 1:, :]))
            
            total_loss = loss + 0.0001 * tv_loss
            
            total_loss.backward()
            optimizer.step()
            
            # Clamp to valid range
            with torch.no_grad():
                reconstructed.clamp_(0, 1)
            
            if (step + 1) % 500 == 0:
                print(f"[*] Iteration {step + 1}/{iterations}, Loss: {loss.item():.4f}")
        
        print(f"[+] Model inversion complete")
        return reconstructed.detach()
    
    def model_extraction_attack(self, target_model: nn.Module, 
                               input_samples: torch.Tensor,
                               surrogate_model: nn.Module,
                               epochs: int = 50) -> ModelStealingResult:
        """
        Model extraction/stealing attack
        Tram√®r et al., 2016
        """
        target_model = target_model.to(self.device)
        surrogate_model = surrogate_model.to(self.device)
        target_model.eval()
        
        print(f"[*] Starting model extraction attack...")
        print(f"[*] Query samples: {len(input_samples)}")
        
        # Query target model to get labels
        with torch.no_grad():
            target_outputs = []
            for batch in input_samples:
                batch = batch.to(self.device)
                output = target_model(batch)
                target_outputs.append(output.cpu())
            target_outputs = torch.cat(target_outputs)
        
        # Train surrogate model
        optimizer = optim.Adam(surrogate_model.parameters(), lr=0.001)
        criterion = nn.MSELoss()
        
        for epoch in range(epochs):
            epoch_loss = 0
            for i, batch in enumerate(input_samples):
                batch = batch.to(self.device)
                labels = target_outputs[i].to(self.device)
                
                optimizer.zero_grad()
                output = surrogate_model(batch)
                loss = criterion(output, labels)
                loss.backward()
                optimizer.step()
                
                epoch_loss += loss.item()
            
            if (epoch + 1) % 10 == 0:
                print(f"[*] Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss:.4f}")
        
        # Evaluate accuracy match
        with torch.no_grad():
            correct = 0
            total = 0
            for batch in input_samples:
                batch = batch.to(self.device)
                target_pred = target_model(batch).argmax(dim=1)
                surrogate_pred = surrogate_model(batch).argmax(dim=1)
                correct += (target_pred == surrogate_pred).sum().item()
                total += batch.size(0)
        
        accuracy = correct / total
        print(f"[+] Model extraction complete - Accuracy match: {accuracy:.2%}")
        
        return ModelStealingResult(
            query_count=len(input_samples),
            accuracy_match=accuracy
        )
